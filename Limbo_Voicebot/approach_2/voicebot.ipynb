{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (2.90)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: deepgram-sdk==3.* in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: openai in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (0.25.2)\n",
      "Requirement already satisfied: websockets>=12.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (12.0)\n",
      "Requirement already satisfied: dataclasses-json>=0.6.3 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (0.6.4)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (4.9.0)\n",
      "Requirement already satisfied: aiohttp>=3.9.1 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (3.9.3)\n",
      "Requirement already satisfied: verboselogs>=1.7 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from deepgram-sdk==3.*) (1.7)\n",
      "Requirement already satisfied: comtypes in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from pyttsx3) (1.3.0)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\arpit\\appdata\\roaming\\python\\python311\\site-packages (from pyttsx3) (306)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from aiohttp>=3.9.1->deepgram-sdk==3.*) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from aiohttp>=3.9.1->deepgram-sdk==3.*) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from aiohttp>=3.9.1->deepgram-sdk==3.*) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from aiohttp>=3.9.1->deepgram-sdk==3.*) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from aiohttp>=3.9.1->deepgram-sdk==3.*) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from dataclasses-json>=0.6.3->deepgram-sdk==3.*) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from dataclasses-json>=0.6.3->deepgram-sdk==3.*) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from httpx>=0.25.2->deepgram-sdk==3.*) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from httpx>=0.25.2->deepgram-sdk==3.*) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->deepgram-sdk==3.*) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\arpit\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\arpit\\appdata\\roaming\\python\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.6.3->deepgram-sdk==3.*) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arpit\\anaconda3\\envs\\arpit-test\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.6.3->deepgram-sdk==3.*) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyttsx3 python-dotenv deepgram-sdk==3.* pydub sounddevice openai termcolor toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "from deepgram import (\n",
    "    DeepgramClient,\n",
    "    PrerecordedOptions,\n",
    "    FileSource,\n",
    ")\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import sounddevice as sd\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import toml\n",
    "from termcolor import colored\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = toml.load(open(\"secrets.toml\", \"r\"))\n",
    "\n",
    "DEEPGRAM_API_KEY = secrets['DEEPGRAM_API_KEY']\n",
    "OPENAI_API_KEY = secrets['OPENAI_API_KEY']\n",
    "PREDEFINED_PROMPT = secrets['PREDEFINED_PROMPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = toml.load(open(\"user_config.toml\", \"r\"))\n",
    "\n",
    "NAME = user_data['NAME']\n",
    "AGE = user_data['AGE']\n",
    "GENDER = user_data['GENDER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_save_to_mp3(file_path=\"recorded_audio.mp3\", duration=4, sample_rate=44100):\n",
    "    try:\n",
    "        audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, dtype='int16')\n",
    "        sd.wait()\n",
    "\n",
    "        audio_segment = AudioSegment(\n",
    "            audio_data.tobytes(),\n",
    "            frame_rate=sample_rate,\n",
    "            sample_width=audio_data.dtype.itemsize,\n",
    "            channels=2\n",
    "        )\n",
    "\n",
    "        audio_segment.export(file_path, format=\"mp3\")\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "\n",
    "def speech_to_text(API_KEY, AUDIO_FILE):\n",
    "    try:\n",
    "        deepgram = DeepgramClient(API_KEY)\n",
    "\n",
    "        with open(AUDIO_FILE, \"rb\") as file:\n",
    "            buffer_data = file.read()\n",
    "\n",
    "        payload: FileSource = {\n",
    "            \"buffer\": buffer_data,\n",
    "        }\n",
    "\n",
    "        options = PrerecordedOptions(\n",
    "            model=\"nova-2\",\n",
    "            smart_format=True,\n",
    "        )\n",
    "\n",
    "        response = deepgram.listen.prerecorded.v(\"1\").transcribe_file(payload, options)\n",
    "\n",
    "        return response.results.channels[0].alternatives[0].transcript\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "def speech_to_text2(AUDIO_FILE):\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    try:\n",
    "        text = recognizer.recognize_google(AUDIO_FILE)\n",
    "        print(f\"You: {text}\")\n",
    "\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        text_list = text.split()\n",
    "    \n",
    "            \n",
    "    except sr.UnknownValueError:\n",
    "        pass\n",
    "    except AssertionError:\n",
    "        print(text_list)\n",
    "\n",
    "def process_text(text, NAME, AGE, GENDER):\n",
    "    if \"time\" in text:\n",
    "        # print(colored(f\"The current time is {datetime.now().strftime('%H:%M')}\", \"green\"))\n",
    "        return f\"It's {datetime.now().strftime('%I:%M %p')} right now.\"\n",
    "    \n",
    "    if text == '' or len(text)<3:\n",
    "        return \"I'm sorry, I didn't catch that.\"\n",
    "    \n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": PREDEFINED_PROMPT.format(NAME, AGE, GENDER)},\n",
    "        {\"role\": \"user\", \"content\": f\"{text}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def text_to_speech(text, voice_id=0):\n",
    "    engine = pyttsx3.init()\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[voice_id].id)\n",
    "    engine.setProperty('rate', 150)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser: Who are you?\u001b[0m\n",
      "\u001b[34mLimbo: Greetings Arpit Sengar, I am Limbo, your personal assistant from Hastakriti. I am here to assist you with any queries or tasks you may have. How may I assist you today?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = record_and_save_to_mp3()\n",
    "    # print(colored(f\"Audio file saved at: {file_path}\\n\", \"yellow\"))\n",
    "    \n",
    "    text = speech_to_text(DEEPGRAM_API_KEY, file_path)\n",
    "    print(colored(f\"User: {text}\", \"green\"))\n",
    "\n",
    "    response = process_text(text, NAME, AGE, GENDER)\n",
    "    print(colored(f\"Limbo: {response}\", \"blue\"))\n",
    "    \n",
    "    text_to_speech(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------- TEST ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser: ya me irÃ­a\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pydub\n",
    "import speech_recognition as sr\n",
    "from termcolor import colored\n",
    "\n",
    "output_ = \"recorded_audio_wav.wav\"\n",
    "\n",
    "def convert_to_wav(input_file, output_file):\n",
    "    audio = pydub.AudioSegment.from_file(input_file)\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "    return output_file\n",
    "\n",
    "def speech_to_text2(AUDIO_FILE, language='en-US'):  # Set default language to English\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        with sr.AudioFile(AUDIO_FILE) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data, language=language)\n",
    "            text_list = text.split()\n",
    "\n",
    "            if not text:\n",
    "                return\n",
    "\n",
    "            return text\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        pass\n",
    "    except AssertionError:\n",
    "        return text_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the language code for the desired language, for example, 'es-ES' for Spanish\n",
    "    file_path = record_and_save_to_mp3(duration=3)\n",
    "    wav_file = convert_to_wav(file_path, output_)\n",
    "    \n",
    "    # Specify the language code for the desired language, for example, 'es-ES' for Spanish\n",
    "    text = speech_to_text2(wav_file, language='es-ES')\n",
    "    \n",
    "    print(colored(f\"User: {text}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708237908.2749238\n",
      "1708237908.2749238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         listen_and_respond(source, recognizer)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 27\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone(device_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mlisten_and_respond\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 10\u001b[0m, in \u001b[0;36mlisten_and_respond\u001b[1;34m(source, recognizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_google(audio)\n",
      "File \u001b[1;32mc:\\Users\\arpit\\anaconda3\\envs\\arpit-test\\Lib\\site-packages\\speech_recognition\\__init__.py:491\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistening timed out while waiting for phrase to start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    493\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\Users\\arpit\\anaconda3\\envs\\arpit-test\\Lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arpit\\anaconda3\\envs\\arpit-test\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def listen_and_respond(source, recognizer):\n",
    "    start_time = time.time()\n",
    "    print(start_time)\n",
    "    while True:\n",
    "        print(time.time())\n",
    "        if time.time() - start_time > 5:\n",
    "            break\n",
    "        \n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"You: {text}\")\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Bot: Silence found, shutting up\")\n",
    "            break\n",
    "\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Bot: Could not request results; {e}\")\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone(device_index=2) as source:\n",
    "        listen_and_respond(source, recognizer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arpit-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
